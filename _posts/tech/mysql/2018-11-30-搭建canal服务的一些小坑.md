---
layout: post
title:  "搭建canal服务的一些小坑"
date:   2018-11-30
tags:
        - 技术
        - canal
        - mysql
        - binlog
        
---


### 1. V1.1.2 版本将 mq.yml 配置合到了 canal.properties 里， 在代码上未妥善处理，

HA+MQ模式下, 如果canal server 发生了主从切换,  虽然从服务的instance能正常接管binlog的拉取处理, 但不能将取得的binlog发往MQ服务.
最新代码中解决
详见 [canal issues #1229](https://github.com/alibaba/canal/issues/1229)



### 2. V1.1.2 release 的 canal.adapter-1.1.2.tar.gz,  spring jar包版本有问题,

client-adapter 启动后报错 java.lang.ClassNotFoundException: org.springframework.core.KotlinDetector

需要把 lib里 3.2.18.RELEASE版本的 spring jar包 换成 5.0.5.RELEASE的

详见 [canal issues #1234](https://github.com/alibaba/canal/issues/1234)

### 3. canal.properties 配置项不是 优美格式
配置 canal.destinations 上线上， 运维把多个 destination 中间加上了空格， 一直报下面的错， 没有明确的指向性。  最后发现是 canal 代码中取得 destinatins 时 对整个字符串进行了trim , 但 split 后没有进行 trim， 这样除了第一个 instance 是正常的， 后面的instance 都没有启来。 


```
2019-08-04 23:58:23.473 [destination = XXX , address = /host:3306 , EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /host:3306 has an error, retrying. caused by 
com.alibaba.otter.canal.parse.exception.CanalParseException: apply failed caused by : Could not get JDBC Connection; nested exception is com.alibaba.druid.pool.DataSourceClosedException: dataSource already closed at Wed Jul 24 15:40:19 CST 2019
Caused by: org.springframework.jdbc.CannotGetJdbcConnectionException: Could not get JDBC Connection; nested exception is com.alibaba.druid.pool.DataSourceClosedException: dataSource already closed at Wed Jul 24 15:40:19 CST 2019
    at org.springframework.orm.ibatis.SqlMapClientTemplate.execute(SqlMapClientTemplate.java:198) ~[spring-orm-3.2.18.RELEASE.jar:3.2.18.RELEASE]
    at org.springframework.orm.ibatis.SqlMapClientTemplate.queryForObject(SqlMapClientTemplate.java:271) ~[spring-orm-3.2.18.RELEASE.jar:3.2.18.RELEASE]
    at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.dao.MetaSnapshotDAO.findByTimestamp(MetaSnapshotDAO.java:29) ~[canal.parse-1.1.2.jar:na]
    at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.buildMemFromSnapshot(DatabaseTableMeta.java:401) ~[canal.parse-1.1.2.jar:na]
    at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:172) ~[canal.parse-1.1.2.jar:na]
    at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) ~[canal.parse-1.1.2.jar:na]
    at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:192) ~[canal.parse-1.1.2.jar:na]
    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111]
Caused by: com.alibaba.druid.pool.DataSourceClosedException: dataSource already closed at Wed Jul 24 15:40:19 CST 2019
    at com.alibaba.druid.pool.DruidDataSource.getConnectionInternal(DruidDataSource.java:1356) ~[druid-1.1.9.jar:1.1.9]
    at com.alibaba.druid.pool.DruidDataSource.getConnectionDirect(DruidDataSource.java:1253) ~[druid-1.1.9.jar:1.1.9]
    at com.alibaba.druid.pool.DruidDataSource.getConnection(DruidDataSource.java:1233) ~[druid-1.1.9.jar:1.1.9]
    at com.alibaba.druid.pool.DruidDataSource.getConnection(DruidDataSource.java:1223) ~[druid-1.1.9.jar:1.1.9]
    at com.alibaba.druid.pool.DruidDataSource.getConnection(DruidDataSource.java:90) ~[druid-1.1.9.jar:1.1.9]
    at org.springframework.jdbc.datasource.DataSourceUtils.doGetConnection(DataSourceUtils.java:111) ~[spring-jdbc-3.2.18.RELEASE.jar:3.2.18.RELEASE]
    at org.springframework.orm.ibatis.SqlMapClientTemplate.execute(SqlMapClientTemplate.java:184) ~[spring-orm-3.2.18.RELEASE.jar:3.2.18.RELEASE]
    ... 7 common frames omitted
2019-08-04 23:58:23.473 [destination = XXX , address = /172.16.4.24:3306 , EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:XXX[com.alibaba.otter.canal.parse.exception.CanalParseException: apply failed caused by : Could not get JDBC Connection; nested exception is com.alibaba.druid.pool.DataSourceClosedException: dataSource already closed at Wed Jul 24 15:40:19 CST 2019
Caused by: org.springframework.jdbc.CannotGetJdbcConnectionException: Could not get JDBC Connection; nested exception is com.alibaba.druid.pool.DataSourceClosedException: dataSource already closed at Wed Jul 24 15:40:19 CST 2019
    at org.springframework.orm.ibatis.SqlMapClientTemplate.execute(SqlMapClientTemplate.java:198)
    at org.springframework.orm.ibatis.SqlMapClientTemplate.queryForObject(SqlMapClientTemplate.java:271)
    at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.dao.MetaSnapshotDAO.findByTimestamp(MetaSnapshotDAO.java:29)
    at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.buildMemFromSnapshot(DatabaseTableMeta.java:401)
    at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:172)
    at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84)
    at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:192)
    at java.lang.Thread.run(Thread.java:745)
Caused by: com.alibaba.druid.pool.DataSourceClosedException: dataSource already closed at Wed Jul 24 15:40:19 CST 2019
    at com.alibaba.druid.pool.DruidDataSource.getConnectionInternal(DruidDataSource.java:1356)
    at com.alibaba.druid.pool.DruidDataSource.getConnectionDirect(DruidDataSource.java:1253)
    at com.alibaba.druid.pool.DruidDataSource.getConnection(DruidDataSource.java:1233)
    at com.alibaba.druid.pool.DruidDataSource.getConnection(DruidDataSource.java:1223)
    at com.alibaba.druid.pool.DruidDataSource.getConnection(DruidDataSource.java:90)
    at org.springframework.jdbc.datasource.DataSourceUtils.doGetConnection(DataSourceUtils.java:111)
    at org.springframework.orm.ibatis.SqlMapClientTemplate.execute(SqlMapClientTemplate.java:184)
    ... 7 more
]
```


### 4.大字段问题
在Kafaka的模式下， kafka broker 配置默认消息最大值是1M，一般也不会改。   如果DB表中有大字段的情况下，很可以一条记录的大小超过1M，无法发送到 kafka broker 上， 造成binlog的消费的阻塞。 

这就要求
1. 在数据库设计的时候，尽量避免大字段， 保持业务库的精简高效
2. 拉取binlog时， 最好启用白名单， 只拉取需要的表的更新， 也可以避免kafka 堆积不必要的数据



